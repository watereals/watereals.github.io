---
layout: post
title: GloVe
mathjax: true
---

# GloVe

Github: [GloVe](https://github.com/stanfordnlp/GloVe)

Standford: [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/)

## Glove算法原理

首先，是一个分析词之间共现概率的例子：

考虑词i和词j是某个特定领域（如：热力学）的词，比如i=ice, j=steam。这里还会用到一个词k，分析$ratio=P_{ik}/P_{jk}$的情况：

词k和ice有关，和steam无关，如k=solid。那么ratio的值会很大。

词k和steam有关，和ice无关，如k=gas。那么ratio的值会很小。

词k和ice, steam均相关/均不相关，如k=water/fashion。那么ratio的值会趋于1。

这个例子想说明什么呢？ 下面是论文中的原话:

> Compared to the raw probabilities, the ratio is better able to distinguish relevant words (solid and gas) from irrelevant words (water and fashion) and it is also better able to discriminate between the two relevant words.

翻译一下，大概是，和原始概率值相比，ratio可以更好的区分相关词(solid, gas)和不相关词(water和fashion)。

比起单纯分析两个词共现概率的值大小，共现概率的比更能说明词与词之间的相关程度。

$$F(w_i, w_j, \tilde{w_k}) = \frac{P_{ik}}{P_{jk}}$$

$$F((w_i-w_j)^T\tilde{w_k}) = \frac{P_{ik}}{P_{jk}}$$

得到

$$F((w_i-w_j)^T\tilde{w_k}) = \frac{F(w_i^T\tilde{w_k})}{F(w_j^T\tilde{w_k})}$$

$$F(w_i^T\tilde{w_k}) = P_{ik} = \frac{X_{ik}}{X_{i}}$$


解得函数

$$F = exp$$

$$w_i^T\tilde{w_k} = log(X_{ik}) - log(X_i)$$

把$X_i$看成一个偏置，再添加一个偏置，得到：

$$w_i^T\tilde{w_k} + b_i + \tilde{b_k} = log(X_{ik})$$

对低频词和高频词设置不同的权重，权重函数定义如下:

$$
f(x)=\begin {cases}
(x/x_{max})^\alpha, & x<x_{max} \\
1, & x\geq{x_{max}}
\end {cases}
$$

推导出损失函数

$$
J 
= \frac{1}{2} \sum_{i,j=1}^V f(X_{ij}) (w_i^T \tilde{w_j} + b_i + \tilde{b_j} - logX_{ij})^2
= \frac{1}{2} \sum_{i,j=1}^V f(X_{ij}) [(\sum_{k=0}^K{w_{ik} \tilde{w_{kj}}}) + b_i + \tilde{b_j} - logX_{ij}]^2
$$

令
$$ e_{ij} = (w_i^T \tilde{w_j} + b_i + \tilde{b_j} - logX_{ij})$$


## code解析

其损失函数与矩阵分解的目标十分接近，按照梯度下降方法求解即可.

梯度下降

$$ \frac {\partial{J}}{\partial{w_{kj}}} = f(X_{ij}) e_{ij} w_{ik} $$

$$ \frac {\partial{J}}{\partial{w_{ik}}} = f(X_{ij}) e_{ij} w_{kj} $$

GloVe算法中使用了Adagrad算法加速梯度下降的求解。
Adagrad算法参考: [梯度下降优化算法综述](http://blog.csdn.net/google19890102/article/details/69942970)

## 一些关于word2vec和GloVe的讨论

[How is GloVe different from word2vec?](https://www.quora.com/How-is-GloVe-different-from-word2vec)

> **Stephan Gouws**: Both models learn geometrical encodings (vectors) of words from their co-occurrence information (how frequently they appear together in large text corpora). They differ in that word2vec is a "predictive" model, whereas GloVe is a "count-based" model. See this paper for more on the distinctions between these two approaches: http://clic.cimec.unitn.it/marco/publications/acl2014/baroni-etal-countpredict-acl2014.pdf

> **Sujit Pal**: With word2vec you stream through n-grams of words, attempting to train a neural network to predict the n-th word given words [1,...,n-1] or the other way round. The end result is a matrix of word vectors or context vectors respectively. With Glove, you build a co-occurrence matrix for the entire corpus first, then factorize it to yield matrices for word vectors and context vectors.

> **Ferhat Aydın**: GloVe论文介绍: [GloVe: Global Vectors for Word Representation](https://blog.acolyer.org/2016/04/22/glove-global-vectors-for-word-representation/), 
word2vec论文介绍: [The amazing power of word vectors](https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/)


## Reference

1. [GloVe模型理解](http://cherishzhang.github.io/post/ml/glove/)

2. glove背后的计算原理(进一步理解glove实现细节)

3. [Matrix Factorization: A Simple Tutorial and Implementation in Python](http://www.quuxlabs.com/blog/2010/09/matrix-factorization-a-simple-tutorial-and-implementation-in-python/)
