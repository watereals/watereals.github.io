---
layout: default
title: Anaconda
---

# GBDT算法原理与应用

论文：Greedy Function Approximation A Gradient Boosting Machine

论文剖析：
梯度提升算法：https://github.com/liudragonfly/GBDT or https://watereals.github.io/gbdt_theory.md

机器学习中的数学(3)-模型组合(Model Combining)之Boosting与Gradient Boosting：http://www.cnblogs.com/LeftNotEasy/archive/2011/01/02/

machine-learning-boosting-and-gradient-boosting.html

简单实现源码：https://github.com/liudragonfly/GBDT

核心源码解释

```python

    if self.loss_type == 'binary-classification':
        self.loss = BinomialDeviance(n_classes=dataset.get_label_size())
    elif self.loss_type == 'regression':
        self.loss = LeastSquaresError(n_classes=1)

    f = dict()  # 记录F_{m-1}的值
    self.loss.initialize(f, dataset)  # f[id][label] = 0
    for iter in range(1, self.max_iter+1):
        subset = train_data

        # 采样
        if 0 < self.sample_rate < 1:
            subset = sample(subset, int(len(subset)*self.sample_rate))

        # 用损失函数的负梯度作为回归问题提升树的残差近似值
        # f是上一步加和的值F_{m-1}，通过f计算拟合的残差（二分类和多分类不同）
        residual = self.loss.compute_residual(dataset, subset, f) # 计算负梯度作为残差
        leaf_nodes = []
        targets = residual

        # 创建决策树，目标是拟合计算出来的残差
        tree = construct_decision_tree(dataset, subset, targets, 0, leaf_nodes, self.max_depth, self.loss, self.split_points)

        # print_tree(tree) # print tree
        self.trees[iter] = tree

        # 更新F_{m-1}的值, F_m = F_{m-1} + x的预测值
        self.loss.update_f_value(f, tree, leaf_nodes, subset, dataset, self.learn_rate)

        if isinstance(self.loss, RegressionLossFunction):
            # todo 判断回归的效果
            pass
        else:
            train_loss = self.compute_loss(dataset, train_data, f)
            print("iter%d : train loss=%f" % (iter,train_loss))
```

其他阅读：

http://www.jianshu.com/p/005a4e6ac775

http://www.52cs.org/?p=429

http://blog.csdn.net/dark_scope/article/details/24863289

http://blog.csdn.net/w28971023/article/details/8240756

http://www.cnblogs.com/LeftNotEasy/archive/2011/03/07/random-forest-and-gbdt.html