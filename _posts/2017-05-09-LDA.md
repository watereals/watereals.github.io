---
layout: post
title: LDA
mathjax: true
---

# LDA算法


## LDA原理


## 用Gibbs Sampling学习LDA

算法伪代码:

**Algorithm** LDAGibbs($\{\vec{w}\},\vec{\alpha}, \vec{\beta}, K$)

**Input**: word vectors $\{\vec{w}\}$. Hyperparameters $\vec{\alpha}, \vec{\beta}$, topic number $K$

**Global data**: count statistics $\{n_m^{(k)}\}, \{n_k^{(v)}\}$ and their sums $\{n_m\},\{n_k\}$, memory for full conditional array $P(z_{m,n} | \vec{w}, \vec{z}_{\neg{\{m,n\}}}, \vec{\alpha}, \vec{\beta} )$

**Output**: topic associations $\{\vec{z}\}$, multinomial parameters $\Phi$ and $\Theta$, hyperparameter estimates $\vec{\alpha}, \vec{\beta}$

// initialization

zero all count variables: $n_m^{(k)}, n_m, n_k^{(v)}, n_k$

**for** all document $m \in [1, M]$ **do**

++++ **for** all words $n \in [1, N_m]$ in document $m$ **do**

++++++++ sample topic index $z_{m,n} = k \sim Multinominal(1/K)$

++++++++ increment document-topic count : $n_m^{(k)} += 1$

++++++++ increment document-topic sum $n_m += 1$

++++++++ increment topic-word count : $n_k^{(v)} += 1$

++++++++ increment topic-word sum $n_k += 1$

// Gibbs sampling over burn-in period and sampling period

**while** not filishedd **do**

++++ **for** all document $m \in [1, M]$ **do**

++++++++ **for** all words $n \in [1, N_m]$ in document $m$ **do**

++++++++++++ // for the current assignment of $k$ to the word token $w_{m,n}$:

++++++++++++ decrement counts and sums: $n_m^{(k)} -= 1,n_m -= 1,n_k^{(v)} -= 1,n_k -= 1$

++++++++++++ //multinomial sampling according to Eq. (2):

++++++++++++ sample topic index $k \sim P(z_{m,n} | \vec{w}, \vec{z}_{\neg{\{m,n\}}}, \vec{\alpha}, \vec{\beta} )$

++++++++++++ // for the current assignment of $k$ to the word token $w_{m,n}$:

++++++++++++ increment counts and sums: $n_m^{(k)} += 1,n_m += 1,n_k^{(v)} += 1,n_k += 1$

++++ **if** converged and $L$ sampling iterations since last read out **then**

++++++++ // the different parameters read outs are averaged.

++++++++ read out parameter set $\Phi$ according to Eq. (5)

++++++++ read out parameter set $\Theta$ according to Eq. (6)

初始时随机给文本中的每个单词分配主题 $z_{m,n} = k \sim Multinominal(1/K)$ ,然后统计每个主题 $z$ 下出现词 $t$ 的数量以及每个文档 $m$ 下出现主题 $z$ 中的词的数量，每一轮计算，即排除当前词的主题分配，根据其他所有词的主题分配估计当前词分配各个主题的概率 $P(z_{m,n} | \vec{w}, \vec{z}_{\neg{\{m,n\}}}, \vec{\alpha}, \vec{\beta} )$ 。当得到当前词属于所有主题 $z$ 的概率分布后，根据这个概率分布为该词sample一个新的主题 $z'$ 。然后用同样的方法不断更新下一个词的主题，直到发现每个文档下Topic分布 $\Theta$ 和每个Topic下词的分布 $\Phi$ 收敛，算法停止，输出待估计的参数 $\Theta$ 和 $\Phi$ ，最终每个单词的主题 $z_{m,n}$ 也同时得出。实际应用中会设置最大迭代次数。每一次计算 $P(z_{m,n} | \vec{w}, \vec{z}_{\neg{\{m,n\}}}, \vec{\alpha}, \vec{\beta} )$ 的公式称为Gibbs updating rule.下面我们来推导LDA的联合分布和Gibbs updating rule。

假设当前文档为$m$，当前词为$v$,那么求解概率为在去掉词$v$后，下一个词抽到$v$的概率。由于可以通过每一个主题$z$来抽取词$v$，因此通过主题$k$来抽取到词$v$的概率为:

$$
P( 通过主题 k 抽到 v| doc - v) \\
        = p(z=k|doc-v) * p(v|z=k) \\
        = \frac{n_m^{(k)} + \alpha}{\sum_{z=1}^K(n_m^{(z)}+\alpha)} * \frac{n_k^{(v)}+\beta}{\sum_{t=1}^V(n_k^{(t)}+\beta)} \\
        = \frac{n_m^{(k)} + \alpha}{n_m+K\alpha} * \frac{n_k^{(v)}+\beta}{n_k+V\beta} \\
        \varpropto (n_m^{(k)} + \alpha) * \frac{n_k^{(v)}+\beta}{n_k+V\beta}
$$
最后一步等价是因为$n_m+K\alpha$与主题无关。
其中,

$n_m$ : 表示文档 $m$ 的词数

$n_k$ : 表示所有文档综合后主题 $z$ 的词数

$n_m^{(k)}$ : 表示文档 $m$ 中，主题 $z=k$ 的词数

$n_k^{(v)}$ : 表示所有文档综合后主题 $z$ 中词 $t=v$ 的词数

对应于lda.py中的采样代码为:
```
p_z = self.n_z_t[:, t] * self.n_m_z[m] / self.n_z
new_zz = numpy.random.multinomial(1, p_z / p_z.sum()).argmax()
```

在求解完成(主题-词分布$\Phi$和文档-主题$\Theta$稳定)后，输出主题-词分布$\Phi$和文档-主题分布$\Theta$:
$$ \Phi_k^v = \frac{n_k^{(v)}+\beta}{n_k+V\beta} $$
$$ \Theta_m^k = \frac{n_m^{(k)} + \alpha}{n_m+K\alpha} $$

## Reference

[概率语言模型及其变形系列(1)-PLSA及EM算法](http://blog.csdn.net/yangliuy/article/details/8330640)

[概率语言模型及其变形系列(2)-LDA及Gibbs Sampling](http://blog.csdn.net/yangliuy/article/details/8302599)

[Topic Model-LDA理论篇](http://nanjunxiao.github.io/2015/08/07/Topic-Model-LDA理论篇/)

[Topic Model-LDA实战篇](http://nanjunxiao.github.io/2015/08/08/Topic-Model-LDA实战篇/)，实战源码：[Github: Three open source versions of LDA with collapsed Gibbs Sampling](https://github.com/nanjunxiao/LDA)

[关于LDA学习的一些有用的博客以及大牛写的代码实现](http://blog.csdn.net/yt71656/article/details/45199603)

[LDA-math-LDA 文本建模](http://www.52nlp.cn/lda-math-lda-文本建模)

博客推荐：[火光摇曳](http://www.flickering.cn/)

