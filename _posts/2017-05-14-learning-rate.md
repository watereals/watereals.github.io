---
layout: post
title: 梯度下降优化算法综述
mathjax: true
---

# 梯度下降优化算法综述

论文: https://arxiv.org/pdf/1609.04747.pdf

介绍: http://sebastianruder.com/optimizing-gradient-descent/

译文: http://blog.csdn.net/google19890102/article/details/69942970

## 1. 摘要
## 2. 引言
## 3. 梯度下降法的变形形式
### 3.1 批梯度下降法
### 3.2 随机梯度下降法
### 3.3 小批量梯度下降法
## 4. 挑战
## 5. 梯度下降优化算法
### 5.1 动量法
### 5.2 Nesterov加速梯度下降法
### 5.3 Adagrad
### 5.4 Adadelta
### 5.5 RMSprop
### 5.6 Adam
### 5.7 算法可视化
### 5.8 选择使用哪种优化算法
## 6. 并行和分布式SGD
### 6.1 Hogwild
### 6.2 Downpour SGD
### 6.3 延迟容忍SGD
### 6.4 TensorFlow
### 6.5 弹性平均SGD
## 7. 优化SGD的其他策略
### 7.1 数据集的洗牌和课程学习
### 7.2 批量归一化
### 7.3 Early stopping
### 7.4 梯度噪音
## 8. 总结
## 9. 参考文献
